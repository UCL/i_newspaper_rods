---
title: Whitechapel Place Names
author: Roma Klapaukh & Raquel Alegre
---


```{r loadLibraries}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(yaml))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(tidytext))
suppressPackageStartupMessages(library(wordcloud))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(parallel))
```

```{r makeCluster}
cl = makeCluster(max(1, detectCores() - 1), type='FORK')
```

This is a short glimpse into the results of the Whitechapel queries. 

In order to try identify place names we created a list of words we thought were likely
to be endings of place names. 

```{r loadSearchTerms}
searchTerms= fread("place_words.csv", header=F) %T>% setnames(c("Suffix"))
```

```{r printSearchTerms}
searchTerms %>%
  arrange(Suffix) %>%
  datatable
```

We can augment the search suffixes into a regex query that will capture some
contexts (up to 2 words) that we hope will be interesting places.

```{r augmentSearchTerms}
prefix="\\b(\\S+\\s*){1,2}\\b"
suffix="\\b"
searchTerms %<>% 
  extract2("Suffix") %>%
  Reduce(function(soFar,term) ifelse(nchar(soFar) == 1, paste0(soFar,term), paste0(soFar,"|",term)) , . ,"(") %>% 
  paste0(.,")") %>%
  paste0(prefix,.,suffix)
```

This gives us one regex that will capture everything that we are looking for:

```{r printRegex}
print(searchTerms)
```

Finally we are ready to load in the search results. This is not the full XML
as in the requested output, but rather we have extracted the text and date from
each article.

```{r readData}
data = yaml.load_file("AllWhiteCbapel.yml")
```

At this point we have a lot of artefacts from the OCR process. 
So we will start by cleaning the data a little bit, and then doing the search.

```{r cleanData}
places = data %>%
  parLapply(cl,.,function(articles) { 
    articles %>% 
        gsub("_+","",.) %>%
        gsub("\\s+"," ",.) %>%
        gsub("[\\/\".?><&%#@!^*=~;:,)(-]"," ",.) %>%
        gsub("\\s+'\\s+"," ", .) %>% 
        map(function(article){
          regmatches(article, gregexpr(searchTerms, article, ignore.case=TRUE,perl=T)) %>%
            sapply(tolower)
        }) %>%
      do.call(c,.)
    }) %>%
  imap(function(place,name){
    if(length(unlist(place)) == 0){
      place = '__DELETE_ME__'
    }
    data.frame(date=ymd_hms(name), place=place %>% unlist)
  }) %>% 
  rbindlist %>%
  filter(place != "__DELETE_ME__")
```

We can have a quick look at the resulting table.
```{r}
places %>%
  head(100) %>%
  datatable
```

It is pretty clear that a lot of the results are pretty meaningless. 
These are likely to be OCR errors, particularly from older newspapers which do not OCR as well. 

In order to get a better idea of which results are likely to be reasonble, let us look
at how often each one occurs. I don't think that looking per year is necessarily a good
idea, so instead we are going to look by decade. 

```{r createTable}
places %<>%
  mutate(decade = 10 * as.integer(year(date) / 10)) %>%
  group_by(decade,place) %>%
  summarise(count = n())
```

And now look at this table

```{r getCounts}
places %>%
  arrange(desc(count), decade) %>%
  head(100) %>% 
  datatable
```


What we have now is a table that has a lot of stop workds in the expressions adding noise. 
Words like of, the, etc. 

Our first attempt is going to be:
  
  * Removing stop words from the expressions
  * Remove all expressions with only 1 word (white space separated, as this will be the original search string)
  * Remove all expressions that appear less than 2 times (as these are almost certainly noise)
  
```{r stopWordsRegex}
allowedWords = c('old','new')
stop_regex <- stop_words %>% 
  filter(!(word %in% allowedWords)) %>% 
  extract2("word") %>%
  unique %>%
  Reduce(function(soFar,term) ifelse(nchar(soFar) == 0, paste0(soFar,term), paste0(soFar,"|",term)) , . ,"") %>% 
  paste0("\\b(",.,")\\b\\s*")
```

```{r stopWordsFilter}
filteredPlaces <- places %>% 
  mutate(place = gsub(stop_regex,"",place)) %>%
  mutate(place = gsub("\\s{2,}"," ",place)) %>%
  mutate(place = str_trim(place)) %>%
  mutate(length = sapply(strsplit(place," "),length)) %>%
  filter(length > 1) %>%
  group_by(decade,place) %>% 
  summarise(count = sum(count)) %>%
  filter(count > 2) %>%
  arrange(desc(count),decade)
```

```{r}
filteredPlaces %>%
  head(100) %>% 
  datatable
```

```{r}
wc <- function(table) { wordcloud(table$place,table$count, scale=c(6,1)) }
wcs = filteredPlaces %>%
  group_by(decade) %>%
  do({
    year = extract2(.,"decade") %>% unique
    png(paste0("images/",year,".png"), width=1024,height=768,units='px') 
    
    layout(matrix(c(1, 2), nrow=2), heights=c(1, 8))
    par(mar=rep(0, 4))
    plot.new()
    text(x=0.5, y=0.5, year, cex=4)
    head(., 100) %>% wc
    dev.off()
  })
```

```{r animate}
system("convert -delay 100 -loop 0 images/*.png images/stack.gif)
```

![Animation of words](images/stack.gif)